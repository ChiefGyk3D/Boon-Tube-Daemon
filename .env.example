# Boon-Tube-Daemon Environment Configuration
# Copy this file to .env and fill in your values
#
# NOTE: Placeholder values starting with 'YOUR_' are automatically ignored.
# This allows you to use Doppler or other secrets managers without errors
# from template values. If a value starts with 'YOUR_', it will be treated
# as if it doesn't exist, allowing proper fallback to secrets managers.

# ============================================================================
# GENERAL SETTINGS
# ============================================================================

# Check interval in seconds (how often to check for new videos)
# Default: 900 (15 minutes) - optimized for video uploads (not livestreams)
# This leaves room for Stream-Daemon to check livestreams every 1-2 minutes
# YouTube API quota: 10,000 units/day, each check = 3 units
#   15 min interval = 96 checks/day = 288 units (safe for both daemons)
CHECK_INTERVAL=900

# Custom notification template (optional)
# Available variables: {platform}, {title}, {url}, {description}
NOTIFICATION_TEMPLATE="ðŸŽ¬ New {platform} video!\n\n{title}\n\n{url}"

# Optional hashtags to append to notifications
HASHTAGS="#NewVideo #Content"

# ============================================================================
# YOUTUBE CONFIGURATION
# ============================================================================

# Enable YouTube monitoring
YOUTUBE_ENABLE_MONITORING=true

# YouTube channel username or handle (with or without @)
# Example: @LinusTechTips or LinusTechTips
YOUTUBE_USERNAME=YOUR_YOUTUBE_CHANNEL

# Optional: Channel ID (faster than username lookup)
# Find it at: https://www.youtube.com/account_advanced
YOUTUBE_CHANNEL_ID=

# YouTube Data API v3 Key
# Get one at: https://console.cloud.google.com/apis/credentials
# Enable YouTube Data API v3 in your project
YOUTUBE_API_KEY=YOUR_YOUTUBE_API_KEY

# ============================================================================
# TIKTOK CONFIGURATION
# ============================================================================

# Enable TikTok monitoring
TIKTOK_ENABLE_MONITORING=false

# TikTok username (without @)
# Example: if URL is @username, just put: username
TIKTOK_USERNAME=YOUR_TIKTOK_USERNAME

# TikTok Official API Credentials (RECOMMENDED - store in Doppler)
# Get these from: https://developers.tiktok.com/
# Create an app and add "Login Kit" product with scopes: user.info.basic,video.list
# These should be stored in Doppler as secrets, not in .env
TIKTOK_CLIENT_KEY=
TIKTOK_CLIENT_SECRET=

# TikTok OAuth Redirect URI (for one-time token setup)
# This is your ngrok HTTPS URL when running scripts/tiktok_oauth.py
# Example: https://abc123.ngrok-free.app/callback
# Register this URL in TikTok Developer Portal â†’ Your App â†’ Login Kit â†’ Redirect URIs
TIKTOK_REDIRECT_URI=

# Note: If official API credentials are not available, falls back to 
# Playwright browser automation (less reliable due to bot detection)

# ============================================================================
# GEMINI AI (LLM) CONFIGURATION
# ============================================================================

# Enable AI features (LLM)
LLM_ENABLE=false

# Choose LLM provider: "gemini" (cloud API) or "ollama" (local server)
# 
# Option 1: Ollama (Local LLM Server - RECOMMENDED for privacy)
#   - Run AI models locally on your own hardware
#   - No API costs, unlimited usage
#   - Full privacy - your data never leaves your network
#   - Fast response times with proper hardware
#   - Setup: https://ollama.com/
#   - Multi-GPU setup: https://github.com/ChiefGyk3D/FrankenLLM
#
# Option 2: Google Gemini (Cloud API - easier setup)
#   - No local hardware required
#   - Pay-per-use API costs (or free tier with limits)
#   - Rate limits apply (15 RPM for free tier)
#   - Setup: Get API key from https://aistudio.google.com/app/apikey
LLM_PROVIDER=ollama

# ===========================================
# OLLAMA CONFIGURATION (LLM_PROVIDER=ollama)
# ===========================================

# Ollama server configuration
# Replace with your Ollama server IP address (or localhost if running locally)
LLM_OLLAMA_HOST=http://192.168.1.100
LLM_OLLAMA_PORT=11434

# Model to use - CHOOSE BASED ON YOUR GPU VRAM
#
# ðŸ† JANUARY 2026 BENCHMARK RESULTS - GEMMA3 WINS!
# Gemma3 models are 6-10x FASTER than Qwen2.5 for notifications (~1s vs ~11s)
#
# RECOMMENDED MODELS BY VRAM:
# | VRAM  | Model          | Quality      | Speed   | Notes                        |
# |-------|----------------|--------------|---------|------------------------------|
# | 4GB   | gemma3:2b      | â­â­â­       | ~1.5s   | Entry-level                  |
# | 6GB   | gemma3:4b      | â­â­â­â­     | ~1.0s ðŸ†| RECOMMENDED - fast & good    |
# | 8GB   | gemma3:4b      | â­â­â­â­â­   | ~1.0s ðŸ†| RECOMMENDED - sweet spot     |
# | 12GB  | gemma3:4b      | â­â­â­â­â­   | ~1.0s   | Fast with headroom           |
# | 16GB  | gemma3:12b     | â­â­â­â­â­+  | ~1.3s ðŸ†| PREMIUM - excellent quality  |
# | 24GB+ | gemma3:27b     | â­â­â­â­â­+  | ~2s     | Maximum quality              |
#
# ALTERNATIVE MODELS (if you prefer Qwen for instruction following):
# - qwen2.5:7b    (~5GB, ~11s) - Excellent instruction following
# - qwen2.5:14b   (~9GB, ~11s) - Premium quality, slower
#
# QWEN3 MODELS (EXPERIMENTAL - requires LLM_ENABLE_THINKING_MODE=True):
# - qwen3:4b (2.5GB) - Has "thinking mode" that shows reasoning
# - qwen3:14b (9.3GB) - Premium with thinking mode
# âš ï¸ Qwen3 outputs reasoning before the actual response. Enable thinking mode if using.
#
# Browse available models: https://ollama.com/library
# List installed: ollama list
# Pull new model: ollama pull gemma3:4b
LLM_MODEL=gemma3:4b

# QWEN3 THINKING MODE SUPPORT (Experimental)
# Qwen3 models output their reasoning in a 'thinking' field before responding.
# Enable this to extract the actual notification from the thinking output.
# LLM_ENABLE_THINKING_MODE=false
# LLM_THINKING_TOKEN_MULTIPLIER=4.0

# OLLAMA SETUP INSTRUCTIONS:
#
# 1. Install Ollama on your server:
#    curl -fsSL https://ollama.com/install.sh | sh
#    
#    Multi-GPU setup? See: https://github.com/ChiefGyk3D/FrankenLLM
#
# 2. Pull a model:
#    ollama pull gemma2:2b
#
# 3. Start Ollama (if not running as service):
#    OLLAMA_HOST=0.0.0.0 ollama serve
#
# 4. Test from Boon-Tube-Daemon host:
#    curl http://YOUR_SERVER_IP:11434/api/tags
#
# 5. Update LLM_OLLAMA_HOST above with your server IP

# ===========================================
# GOOGLE GEMINI CONFIGURATION (LLM_PROVIDER=gemini)
# ===========================================

# Gemini API key from Google AI Studio
# Get one at: https://makersuite.google.com/app/apikey
LLM_GEMINI_API_KEY=

# Model to use
# Options (for Gemini):
#   gemini-2.5-flash-lite - 1,000 requests/day, 15 req/min (recommended, default)
#   gemini-2.0-flash-exp  - 50 requests/day (experimental, latest features but low quota)
# Options (for Ollama):
#   See LLM_MODEL setting in Ollama section above
# Note: When using LLM_PROVIDER=gemini, set to a gemini model; when using ollama, set in LLM_MODEL above
LLM_MODEL=gemini-2.5-flash-lite

# Rate limiting for Gemini API (requests per minute)
# Free tier: 15 req/min recommended (default)
# Paid tier: Increase based on your quota
LLM_RATE_LIMIT=15

# Delay between platform posts (seconds) to space out LLM API calls
# Prevents hitting rate limits when posting to multiple platforms
# Default: 2.0 seconds between each platform's LLM call
# Set to 0 to disable (not recommended if using multiple platforms)
LLM_PLATFORM_DELAY=2.0

# AI-enhanced notifications (generates unique posts per platform)
# When enabled, each platform (Discord, Matrix, Bluesky, Mastodon) gets a
# custom post tailored to its audience and style:
#   - Discord/Matrix: No hashtags, conversational
#   - Bluesky/Mastodon: With hashtags, optimized length
# The LLM also cleans video descriptions (removes sponsors, URLs, etc.)
LLM_ENHANCE_NOTIFICATIONS=false

# Auto-generate hashtags based on content (deprecated - now per-platform)
# Hashtags are now automatically included for Bluesky/Mastodon when
# LLM_ENHANCE_NOTIFICATIONS is enabled
LLM_GENERATE_HASHTAGS=false

# Enable content filtering (skip spam/low-quality videos)
LLM_ENABLE_FILTERING=false

# Keywords to filter out (comma-separated)
LLM_FILTER_KEYWORDS=spam,clickbait,scam

# ============================================================================
# PLATFORM POSTING STYLE CONFIGURATION
# ============================================================================
# Configure the tone/style for each platform when LLM is enabled
# Options: professional, conversational, detailed, concise
#
# - professional: Formal, clear, business-like tone
# - conversational: Casual, friendly, community-focused
# - detailed: Comprehensive with context and explanation
# - concise: Brief, to-the-point, minimal text
#
# Default styles if not configured:
#   Discord: conversational
#   Matrix: professional
#   Bluesky: conversational
#   Mastodon: detailed

# Discord posting style (conversational by default)
DISCORD_POST_STYLE=conversational

# Matrix posting style (professional by default)
MATRIX_POST_STYLE=professional

# Bluesky posting style (conversational by default)
BLUESKY_POST_STYLE=conversational

# Mastodon posting style (detailed by default)
MASTODON_POST_STYLE=detailed

# ============================================================================
# DISCORD CONFIGURATION
# ============================================================================

# Enable Discord notifications
DISCORD_ENABLE_POSTING=false

# Default Discord webhook URL (used for all platforms unless overridden)
# Create webhook in Server Settings -> Integrations -> Webhooks
DISCORD_WEBHOOK_URL=YOUR_DISCORD_WEBHOOK_URL

# Optional: Platform-specific webhooks (overrides default)
# Useful if you want different channels for different content types
DISCORD_WEBHOOK_YOUTUBE=
DISCORD_WEBHOOK_TIKTOK=

# Optional: Default role ID to mention (for @role notifications)
# Enable Developer Mode, right-click role -> Copy ID
DISCORD_ROLE=

# Optional: Platform-specific role IDs (overrides default)
# Perfect for separate YouTube and TikTok alert roles
DISCORD_ROLE_YOUTUBE=
DISCORD_ROLE_TIKTOK=

# ============================================================================
# MATRIX CONFIGURATION
# ============================================================================

# Enable Matrix notifications
MATRIX_ENABLE_POSTING=false

# Matrix homeserver URL
# Example: https://matrix.org or https://matrix.example.com
MATRIX_HOMESERVER=https://matrix.org

# Matrix room ID (without server suffix)
# Example: !abcdefghijklmnop1234567890
# Find in Room Settings -> Advanced (use only the part before the colon)
MATRIX_ROOM_ID=

# Matrix authentication
# Option 1: Access token (recommended for bots)
MATRIX_ACCESS_TOKEN=

# Option 2: Username and password (will auto-login)
MATRIX_USERNAME=
MATRIX_PASSWORD=

# ============================================================================
# BLUESKY CONFIGURATION
# ============================================================================

# Enable Bluesky notifications
BLUESKY_ENABLE_POSTING=false

# Bluesky handle (your username)
# Example: username.bsky.social
BLUESKY_HANDLE=YOUR_HANDLE.bsky.social

# Bluesky app password (NOT your main password!)
# Create at: Settings -> App Passwords
BLUESKY_APP_PASSWORD=

# ============================================================================
# MASTODON CONFIGURATION
# ============================================================================

# Enable Mastodon notifications
MASTODON_ENABLE_POSTING=false

# Mastodon instance URL
# Example: https://mastodon.social
MASTODON_API_BASE_URL=https://mastodon.social

# Mastodon application credentials
# Create app at: Settings -> Development -> New Application
MASTODON_CLIENT_ID=YOUR_CLIENT_ID
MASTODON_CLIENT_SECRET=YOUR_CLIENT_SECRET
MASTODON_ACCESS_TOKEN=YOUR_ACCESS_TOKEN

# ============================================================================
# ADVANCED: SECRET MANAGEMENT (Optional)
# ============================================================================
# All secret names use the same format: SECTION_KEY (e.g., YOUTUBE_API_KEY)
# The system automatically checks secret managers first, then falls back to .env
#
# Priority order:
# 1. Doppler (if DOPPLER_TOKEN is set)
# 2. AWS Secrets Manager (if SECRETS_AWS_ENABLED=true)
# 3. HashiCorp Vault (if SECRETS_VAULT_ENABLED=true)
# 4. Environment variables / .env file
#
# This means you can use the same secret names everywhere - the system
# will automatically check Doppler/AWS/Vault first, then fall back to .env

# Doppler Configuration
# ---------------------
# When DOPPLER_TOKEN is set, all secrets are automatically loaded from Doppler
# Get your token from: https://dashboard.doppler.com/ â†’ Access â†’ Service Tokens
# Or run with Doppler CLI: doppler run -- python3 main.py
#
# Example:
#   DOPPLER_TOKEN=dp.st.your_token_here
#
# When using Doppler, all secrets use standard names: YOUTUBE_API_KEY, DISCORD_WEBHOOK_URL, etc.
DOPPLER_TOKEN=

# AWS Secrets Manager Configuration
# ----------------------------------
# Enable AWS Secrets Manager for secret retrieval
SECRETS_AWS_ENABLED=false

# Base secret name (secrets stored as: <base>/<section> e.g., boon-tube/youtube)
# Each section (YouTube, Discord, etc.) is stored as a separate secret
SECRETS_AWS_SECRET_NAME=boon-tube

# HashiCorp Vault Configuration
# ------------------------------
# Enable HashiCorp Vault for secret retrieval
SECRETS_VAULT_ENABLED=false
SECRETS_VAULT_URL=https://vault.example.com:8200
SECRETS_VAULT_TOKEN=your_vault_token

# Base path (secrets stored at: <path>/<section> e.g., secret/boon-tube/youtube)
# Each section (YouTube, Discord, etc.) is stored as a separate path
SECRETS_VAULT_PATH=secret/boon-tube
