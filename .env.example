# Boon-Tube-Daemon Environment Configuration
# Copy this file to .env and fill in your values
#
# NOTE: Placeholder values starting with 'YOUR_' are automatically ignored.
# This allows you to use Doppler or other secrets managers without errors
# from template values. If a value starts with 'YOUR_', it will be treated
# as if it doesn't exist, allowing proper fallback to secrets managers.

# ============================================================================
# GENERAL SETTINGS
# ============================================================================

# Check interval in seconds (how often to check for new videos)
# Default: 900 (15 minutes) - optimized for video uploads (not livestreams)
# This leaves room for Stream-Daemon to check livestreams every 1-2 minutes
# YouTube API quota: 10,000 units/day, each check = 3 units
#   15 min interval = 96 checks/day = 288 units (safe for both daemons)
CHECK_INTERVAL=900

# Custom notification template (optional)
# Available variables: {platform}, {title}, {url}, {description}
NOTIFICATION_TEMPLATE="ðŸŽ¬ New {platform} video!\n\n{title}\n\n{url}"

# Optional hashtags to append to notifications
HASHTAGS="#NewVideo #Content"

# ============================================================================
# YOUTUBE CONFIGURATION
# ============================================================================

# Enable YouTube monitoring
YOUTUBE_ENABLE_MONITORING=true

# YouTube channel username or handle (with or without @)
# Example: @LinusTechTips or LinusTechTips
YOUTUBE_USERNAME=YOUR_YOUTUBE_CHANNEL

# Optional: Channel ID (faster than username lookup)
# Find it at: https://www.youtube.com/account_advanced
YOUTUBE_CHANNEL_ID=

# YouTube Data API v3 Key
# Get one at: https://console.cloud.google.com/apis/credentials
# Enable YouTube Data API v3 in your project
YOUTUBE_API_KEY=YOUR_YOUTUBE_API_KEY

# ============================================================================
# TIKTOK CONFIGURATION
# ============================================================================

# Enable TikTok monitoring
TIKTOK_ENABLE_MONITORING=false

# TikTok username (without @)
# Example: if URL is @username, just put: username
TIKTOK_USERNAME=YOUR_TIKTOK_USERNAME

# TikTok Official API Credentials (RECOMMENDED - store in Doppler)
# Get these from: https://developers.tiktok.com/
# Create an app and add "Login Kit" product with scopes: user.info.basic,video.list
# These should be stored in Doppler as secrets, not in .env
TIKTOK_CLIENT_KEY=
TIKTOK_CLIENT_SECRET=

# TikTok OAuth Redirect URI (for one-time token setup)
# This is your ngrok HTTPS URL when running scripts/tiktok_oauth.py
# Example: https://abc123.ngrok-free.app/callback
# Register this URL in TikTok Developer Portal â†’ Your App â†’ Login Kit â†’ Redirect URIs
TIKTOK_REDIRECT_URI=

# Note: If official API credentials are not available, falls back to 
# Playwright browser automation (less reliable due to bot detection)

# ============================================================================
# GEMINI AI (LLM) CONFIGURATION
# ============================================================================

# Enable AI features (LLM)
LLM_ENABLE=false

# Choose LLM provider: "gemini" (cloud API) or "ollama" (local server)
# 
# Option 1: Ollama (Local LLM Server - RECOMMENDED for privacy)
#   - Run AI models locally on your own hardware
#   - No API costs, unlimited usage
#   - Full privacy - your data never leaves your network
#   - Fast response times with proper hardware
#   - Setup: https://ollama.com/
#   - Multi-GPU setup: https://github.com/ChiefGyk3D/FrankenLLM
#
# Option 2: Google Gemini (Cloud API - easier setup)
#   - No local hardware required
#   - Pay-per-use API costs (or free tier with limits)
#   - Rate limits apply (15 RPM for free tier)
#   - Setup: Get API key from https://aistudio.google.com/app/apikey
LLM_PROVIDER=ollama

# ===========================================
# OLLAMA CONFIGURATION (LLM_PROVIDER=ollama)
# ===========================================

# Ollama server configuration
# Replace with your Ollama server IP address (or localhost if running locally)
LLM_OLLAMA_HOST=http://192.168.1.100
LLM_OLLAMA_PORT=11434

# Model to use - CHOOSE BASED ON YOUR GPU VRAM
#
# ðŸ† JANUARY 2026 BENCHMARK RESULTS - GEMMA3 WINS!
# Gemma3 models are 6-10x FASTER than Qwen2.5 for notifications (~1s vs ~11s)
#
# RECOMMENDED MODELS BY VRAM:
# | VRAM  | Model          | Quality      | Speed   | Notes                        |
# |-------|----------------|--------------|---------|------------------------------|
# | 4GB   | gemma3:2b      | â­â­â­       | ~1.5s   | Entry-level                  |
# | 6GB   | gemma3:4b      | â­â­â­â­     | ~1.0s ðŸ†| RECOMMENDED - fast & good    |
# | 8GB   | gemma3:4b      | â­â­â­â­â­   | ~1.0s ðŸ†| RECOMMENDED - sweet spot     |
# | 12GB  | gemma3:4b      | â­â­â­â­â­   | ~1.0s   | Fast with headroom           |
# | 16GB  | gemma3:12b     | â­â­â­â­â­+  | ~1.3s ðŸ†| PREMIUM - excellent quality  |
# | 24GB+ | gemma3:27b     | â­â­â­â­â­+  | ~2s     | Maximum quality              |
#
# ALTERNATIVE MODELS (if you prefer Qwen for instruction following):
# - qwen2.5:7b    (~5GB, ~11s) - Excellent instruction following
# - qwen2.5:14b   (~9GB, ~11s) - Premium quality, slower
#
# QWEN3 MODELS (EXPERIMENTAL - requires LLM_ENABLE_THINKING_MODE=True):
# - qwen3:4b (2.5GB) - Has "thinking mode" that shows reasoning
# - qwen3:14b (9.3GB) - Premium with thinking mode
# âš ï¸ Qwen3 outputs reasoning before the actual response. Enable thinking mode if using.
#
# Browse available models: https://ollama.com/library
# List installed: ollama list
# Pull new model: ollama pull gemma3:4b
LLM_MODEL=gemma3:4b

# ===========================================
# LLM GENERATION PARAMETERS
# ===========================================
# Fine-tune AI output for better instruction following
# "Because apparently machines need parameters to tell them how creative to be.
#  Humans just wing it. Machines need a PowerPoint presentation first."

# Temperature: Controls randomness (0.0 = deterministic, 1.0 = creative)
# Lower = more predictable, follows instructions better
# Higher = more creative, might ignore your carefully crafted prompts
# Recommended: 0.3 for notifications (you want consistency, not jazz)
# Default: 0.3
LLM_TEMPERATURE=0.3

# Top P: Nucleus sampling threshold (0.0-1.0)
# Controls diversity of token selection
# 0.9 = considers 90% of probability mass (good balance)
# 1.0 = considers everything (chaotic)
# Recommended: 0.9 (trust the math)
# Default: 0.9
LLM_TOP_P=0.9

# Max Tokens: Maximum length of generated output
# Higher = more verbose, lower = more concise
# For notifications: 100-200 is plenty (it's a notification, not a novel)
# For thinking mode: multiply by LLM_THINKING_TOKEN_MULTIPLIER
# Default: 150
LLM_MAX_TOKENS=150

# ===========================================
# QWEN3 THINKING MODE SUPPORT (Experimental)
# ===========================================
# Qwen3 models have a unique "thinking" mode where they output their reasoning
# in a separate 'thinking' field before generating the final response.
#
# "It's like having a coworker who explains their entire thought process
#  before answering 'yes' or 'no'. Useful? Sometimes. Efficient? Never."
#
# When enabled, Boon-Tube-Daemon will:
# 1. Increase token limit to allow for reasoning (multiplied by thinking_token_multiplier)
# 2. Extract the actual post from the thinking output
# 3. Handle cases where content field is empty but thinking field has the answer
#
# IMPORTANT: This is experimental. If you're using Qwen3 models, enable this.
# For non-thinking models (gemma3, qwen2.5, llama3, etc.), leave disabled.
#
# Default: false
LLM_ENABLE_THINKING_MODE=false

# Token multiplier for thinking mode
# Qwen3 needs more tokens to complete its reasoning before outputting the answer
# The base max_tokens is multiplied by this value when thinking mode is enabled
# Recommended: 4.0-6.0 (150 tokens * 4 = 600 tokens for thinking + response)
# Default: 4.0
LLM_THINKING_TOKEN_MULTIPLIER=4.0

# ===========================================
# LLM GUARDRAILS & QUALITY CONTROLS
# ===========================================
# Because even robots need rules. We built machines that can think,
# now we need machines to watch the thinking machines.
# It's supervision turtles all the way down.
#
# IMPORTANT: These guardrails control WHAT THE AI GENERATES, not the code commentary.
# The AI generates normal, professional video notifications.
# George Carlin's humor is in the CODE, not in your "New video!" posts.

# Message Deduplication: Prevent identical messages across platforms (true/false)
# If enabled, caches recent messages and forces retry if duplicate detected
# Useful when posting to multiple platforms simultaneously
# 
# "Oh no, the AI used the same words twice! Better waste CPU cycles fixing that."
# Although to be fair, unique messages for each platform IS the point.
# Default: true
LLM_ENABLE_DEDUPLICATION=true

# Deduplication cache size (how many recent messages to remember)
# Higher = better dedup, more memory usage
# Default: 20 (covers ~10 videos worth of platform posts)
LLM_DEDUP_CACHE_SIZE=20

# Quality Scoring: Rate generated messages 1-10 and retry if too low (true/false)
# Checks for: generic words, poor grammar, lack of personality, too short/long
# 
# "We're literally grading the AI's homework. In 2026. This is fine."
# Default: false (enable if you're picky about quality)
LLM_ENABLE_QUALITY_SCORING=false

# Minimum quality score (1-10) to accept a message
# Below this = automatic retry with stricter prompt
# Recommended: 6-7 (not too strict, not too lenient)
# Default: 6
LLM_MIN_QUALITY_SCORE=6

# Emoji Limits: Maximum emoji count in generated messages (0-10)
# Too many emojis = looks like a teenager's Instagram post
# Too few = boring corporate speak
# The sweet spot: 0-1 emoji. Maybe 2 if you're feeling wild.
# Default: 1
LLM_MAX_EMOJI_COUNT=1

# Profanity Filter: Block generated messages with profanity (true/false)
# 
# Note: This blocks GENERATED NOTIFICATIONS, not our beautiful code commentary.
# The code can curse. Your "New video!" posts? That's between you and your audience.
# Default: false (we're adults here)
LLM_ENABLE_PROFANITY_FILTER=false

# Profanity severity threshold (mild/moderate/severe)
# mild: blocks hard profanity only
# moderate: + sexual references, slurs
# severe: + mild profanity (damn, hell, crap)
# Default: moderate
LLM_PROFANITY_SEVERITY=moderate

# Platform-Specific Validation: Extra checks per platform (true/false)
# Discord: Validates mentions, embeds, markdown
# Bluesky: Validates facets, link cards, AT Protocol formatting
# Mastodon: Validates CW, visibility, HTML entities
# 
# "Because one set of rules wasn't enough. Every platform needs special treatment."
# Default: true
LLM_ENABLE_PLATFORM_VALIDATION=true

# ===========================================
# LLM RETRY CONFIGURATION
# ===========================================
# Handles transient API errors (503 overload, timeouts, etc.)
# Because servers have bad days too.

# Maximum number of retry attempts for API calls
# Default: 3 (enough to handle temporary issues, not enough to annoy you)
LLM_MAX_RETRIES=3

# Base delay for exponential backoff in seconds
# Actual delays: 2s, 4s, 8s for attempts 1, 2, 3
# Default: 2
LLM_RETRY_DELAY_BASE=2

# OLLAMA SETUP INSTRUCTIONS:
#
# 1. Install Ollama on your server:
#    curl -fsSL https://ollama.com/install.sh | sh
#    
#    Multi-GPU setup? See: https://github.com/ChiefGyk3D/FrankenLLM
#
# 2. Pull a model (based on your VRAM - see table above):
#    ollama pull gemma3:4b    # For 6GB GPUs (recommended - fast!)
#    ollama pull gemma3:12b   # For 16GB GPUs (excellent quality)
#
# 3. Start Ollama (if not running as service):
#    OLLAMA_HOST=0.0.0.0 ollama serve
#
# 4. Test from Boon-Tube-Daemon host:
#    curl http://YOUR_SERVER_IP:11434/api/tags
#
# 5. Update LLM_OLLAMA_HOST above with your server IP
#
# For detailed model recommendations, see: docs/features/llm-model-recommendations.md

# ===========================================
# GOOGLE GEMINI CONFIGURATION (LLM_PROVIDER=gemini)
# ===========================================

# Gemini API key from Google AI Studio
# Get one at: https://makersuite.google.com/app/apikey
LLM_GEMINI_API_KEY=

# Model to use
# Options (for Gemini):
#   gemini-2.5-flash-lite - 1,000 requests/day, 15 req/min (recommended, default)
#   gemini-2.0-flash-exp  - 50 requests/day (experimental, latest features but low quota)
# Options (for Ollama):
#   See LLM_MODEL setting in Ollama section above
# Note: When using LLM_PROVIDER=gemini, set to a gemini model; when using ollama, set in LLM_MODEL above
LLM_MODEL=gemini-2.5-flash-lite

# Rate limiting for Gemini API (requests per minute)
# Free tier: 15 req/min recommended (default)
# Paid tier: Increase based on your quota
LLM_RATE_LIMIT=15

# Delay between platform posts (seconds) to space out LLM API calls
# Prevents hitting rate limits when posting to multiple platforms
# Default: 2.0 seconds between each platform's LLM call
# Set to 0 to disable (not recommended if using multiple platforms)
LLM_PLATFORM_DELAY=2.0

# AI-enhanced notifications (generates unique posts per platform)
# When enabled, each platform (Discord, Matrix, Bluesky, Mastodon) gets a
# custom post tailored to its audience and style:
#   - Discord/Matrix: No hashtags, conversational
#   - Bluesky/Mastodon: With hashtags, optimized length
# The LLM also cleans video descriptions (removes sponsors, URLs, etc.)
LLM_ENHANCE_NOTIFICATIONS=false

# Auto-generate hashtags based on content (deprecated - now per-platform)
# Hashtags are now automatically included for Bluesky/Mastodon when
# LLM_ENHANCE_NOTIFICATIONS is enabled
LLM_GENERATE_HASHTAGS=false

# Enable content filtering (skip spam/low-quality videos)
LLM_ENABLE_FILTERING=false

# Keywords to filter out (comma-separated)
LLM_FILTER_KEYWORDS=spam,clickbait,scam

# ============================================================================
# PLATFORM POSTING STYLE CONFIGURATION
# ============================================================================
# Configure the tone/style for each platform when LLM is enabled
# Options: professional, conversational, detailed, concise
#
# - professional: Formal, clear, business-like tone
# - conversational: Casual, friendly, community-focused
# - detailed: Comprehensive with context and explanation
# - concise: Brief, to-the-point, minimal text
#
# Default styles if not configured:
#   Discord: conversational
#   Matrix: professional
#   Bluesky: conversational
#   Mastodon: detailed

# Discord posting style (conversational by default)
DISCORD_POST_STYLE=conversational

# Matrix posting style (professional by default)
MATRIX_POST_STYLE=professional

# Bluesky posting style (conversational by default)
BLUESKY_POST_STYLE=conversational

# Mastodon posting style (detailed by default)
MASTODON_POST_STYLE=detailed

# ============================================================================
# DISCORD CONFIGURATION
# ============================================================================

# Enable Discord notifications
DISCORD_ENABLE_POSTING=false

# Default Discord webhook URL (used for all platforms unless overridden)
# Create webhook in Server Settings -> Integrations -> Webhooks
DISCORD_WEBHOOK_URL=YOUR_DISCORD_WEBHOOK_URL

# Optional: Platform-specific webhooks (overrides default)
# Useful if you want different channels for different content types
DISCORD_WEBHOOK_YOUTUBE=
DISCORD_WEBHOOK_TIKTOK=

# Optional: Default role ID to mention (for @role notifications)
# Enable Developer Mode, right-click role -> Copy ID
DISCORD_ROLE=

# Optional: Platform-specific role IDs (overrides default)
# Perfect for separate YouTube and TikTok alert roles
DISCORD_ROLE_YOUTUBE=
DISCORD_ROLE_TIKTOK=

# ============================================================================
# MATRIX CONFIGURATION
# ============================================================================

# Enable Matrix notifications
MATRIX_ENABLE_POSTING=false

# Matrix homeserver URL
# Example: https://matrix.org or https://matrix.example.com
MATRIX_HOMESERVER=https://matrix.org

# Matrix room ID (without server suffix)
# Example: !abcdefghijklmnop1234567890
# Find in Room Settings -> Advanced (use only the part before the colon)
MATRIX_ROOM_ID=

# Matrix authentication
# Option 1: Access token (recommended for bots)
MATRIX_ACCESS_TOKEN=

# Option 2: Username and password (will auto-login)
MATRIX_USERNAME=
MATRIX_PASSWORD=

# ============================================================================
# BLUESKY CONFIGURATION
# ============================================================================

# Enable Bluesky notifications
BLUESKY_ENABLE_POSTING=false

# Bluesky handle (your username)
# Example: username.bsky.social
BLUESKY_HANDLE=YOUR_HANDLE.bsky.social

# Bluesky app password (NOT your main password!)
# Create at: Settings -> App Passwords
BLUESKY_APP_PASSWORD=

# ============================================================================
# MASTODON CONFIGURATION
# ============================================================================

# Enable Mastodon notifications
MASTODON_ENABLE_POSTING=false

# Mastodon instance URL
# Example: https://mastodon.social
MASTODON_API_BASE_URL=https://mastodon.social

# Mastodon application credentials
# Create app at: Settings -> Development -> New Application
MASTODON_CLIENT_ID=YOUR_CLIENT_ID
MASTODON_CLIENT_SECRET=YOUR_CLIENT_SECRET
MASTODON_ACCESS_TOKEN=YOUR_ACCESS_TOKEN

# ============================================================================
# ADVANCED: SECRET MANAGEMENT (Optional)
# ============================================================================
# All secret names use the same format: SECTION_KEY (e.g., YOUTUBE_API_KEY)
# The system automatically checks secret managers first, then falls back to .env
#
# Priority order:
# 1. Doppler (if DOPPLER_TOKEN is set)
# 2. AWS Secrets Manager (if SECRETS_AWS_ENABLED=true)
# 3. HashiCorp Vault (if SECRETS_VAULT_ENABLED=true)
# 4. Environment variables / .env file
#
# This means you can use the same secret names everywhere - the system
# will automatically check Doppler/AWS/Vault first, then fall back to .env

# Doppler Configuration
# ---------------------
# When DOPPLER_TOKEN is set, all secrets are automatically loaded from Doppler
# Get your token from: https://dashboard.doppler.com/ â†’ Access â†’ Service Tokens
# Or run with Doppler CLI: doppler run -- python3 main.py
#
# Example:
#   DOPPLER_TOKEN=dp.st.your_token_here
#
# When using Doppler, all secrets use standard names: YOUTUBE_API_KEY, DISCORD_WEBHOOK_URL, etc.
DOPPLER_TOKEN=

# AWS Secrets Manager Configuration
# ----------------------------------
# Enable AWS Secrets Manager for secret retrieval
SECRETS_AWS_ENABLED=false

# Base secret name (secrets stored as: <base>/<section> e.g., boon-tube/youtube)
# Each section (YouTube, Discord, etc.) is stored as a separate secret
SECRETS_AWS_SECRET_NAME=boon-tube

# HashiCorp Vault Configuration
# ------------------------------
# Enable HashiCorp Vault for secret retrieval
SECRETS_VAULT_ENABLED=false
SECRETS_VAULT_URL=https://vault.example.com:8200
SECRETS_VAULT_TOKEN=your_vault_token

# Base path (secrets stored at: <path>/<section> e.g., secret/boon-tube/youtube)
# Each section (YouTube, Discord, etc.) is stored as a separate path
SECRETS_VAULT_PATH=secret/boon-tube
